### 1、系统规划

| 系统类型 | IP地址	| 节点角色 | 主机名 |
| :-- | :-- | :-- | :-- | :-- | :-- |
| centos-7.2 | 192.168.0.2 | master（api-server/etcd/flannel/controller-manager/scheduler/keepalived） | master1 |
| centos-7.2 | 192.168.0.3 | master（api-server/etcd/flannel/controller-manager/scheduler/keepalived） | master2 |
| centos-7.2 | 192.168.0.4 | master（api-server/etcd/flannel/controller-manager/scheduler) | master3 |
| centos-7.2 | 192.168.0.5 | worker(kubelet/kube-proxy) | worker1 |
| centos-7.2 | 192.168.0.6 | worker(kubelet/kube-proxy) | worker2 |

### 2、系统设置（所有节点）

* 2.1、主机名
主机名必须每个节点都不一样，并且保证所有点之间可以通过hostname互相访问。

```
# 查看主机名
hostname
# 修改主机名
hostnamectl set-hostname master1
hostnamectl set-hostname master2
hostnamectl set-hostname master3
hostnamectl set-hostname worker1
hostnamectl set-hostname worker2
vi /etc/hosts
192.168.0.2 master1
192.168.0.3 master2
192.168.0.4 master3
192.168.0.5 worker1
192.168.0.6 worker2
```

* 2.2、安装依赖包

```
# 更新yum
yum update
# 安装依赖包
yum install -y conntrack ipvsadm ipset jq sysstat curl iptables libseccomp wget net-tools lrzsz git
```

* 2.3、关闭防火墙、swap，重置iptables

```
# 关闭防火墙
systemctl stop firewalld && systemctl disable firewalld

# 重置iptables
iptables -F && iptables -X && iptables -F -t nat && iptables -X -t nat && iptables -P FORWARD ACCEPT

# 关闭swap
swapoff -a
sed -i '/swap/s/^\(.*\)$/#\1/g' /etc/fstab

# 关闭selinux
setenforce 0

# 关闭dnsmasq(否则可能导致docker容器无法解析域名)
service dnsmasq stop && systemctl disable dnsmasq
```

* 2.4、系统参数设置

```
# 制作配置文件
cat > /etc/sysctl.d/kubernetes.conf <<EOF
net.bridge.bridge-nf-call-iptables=1
net.bridge.bridge-nf-call-ip6tables=1
net.ipv4.ip_forward=1
vm.swappiness=0
vm.overcommit_memory=1
vm.panic_on_oom=0
fs.inotify.max_user_watches=89100
EOF

# 生效文件
sysctl -p /etc/sysctl.d/kubernetes.conf
```

### 3、安装Docker

```
# 手动下载rpm包
mkdir -p /opt/kubernetes/docker && cd /opt/kubernetes/docker
wget http://yum.dockerproject.org/repo/main/centos/7/Packages/docker-engine-selinux-17.03.1.ce-1.el7.centos.noarch.rpm

wget http://yum.dockerproject.org/repo/main/centos/7/Packages/docker-engine-17.03.1.ce-1.el7.centos.x86_64.rpm

wget http://yum.dockerproject.org/repo/main/centos/7/Packages/docker-engine-debuginfo-17.03.1.ce-1.el7.centos.x86_64.rpm

# 清理原有版本
yum remove -y docker* container-selinux

# 安装rpm包
yum localinstall -y *.rpm

# 开机启动
systemctl enable docker

# 设置参数
# 1.查看磁盘挂载
df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda2        98G  2.8G   95G   3% /
devtmpfs         63G     0   63G   0% /dev
/dev/sda5      1015G  8.8G 1006G   1% /tol
/dev/sda1       197M  161M   37M  82% /boot

# 2.选择比较大的分区（我这里是/tol）
mkdir -p /tol/docker-data
cat <<EOF > /etc/docker/daemon.json
{
    "graph": "/tol/docker-data"
}
EOF

# 启动docker服务
service docker restart
```

### 4、准备二进制文件

* 4.1、免密登录

```
# 看看是否已经存在rsa公钥
cat ~/.ssh/id_rsa.pub

# 如果不存在就创建一个新的
ssh-keygen -t rsa

# 把id_rsa.pub文件内容copy到其他机器的授权文件中
cat ~/.ssh/id_rsa.pub

# 所有节点执行
mkdir -p /root/.ssh

# 在其他节点执行下面命令（包括worker节点）
echo "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC0klXH+IYWAF3Jnd6kac4VNQk0nh9QQnH9do9mbUjaMsAqMPsfCyCu9tk6G/3Pf+AJOKR0jcWBb0YI0u/+2hjUui96e+cj7sm0XoF2gfFKRm2sCcrr6JG7eufaT+mqGLZRjRWg9wBkHzpBne4lsw9m82MUqo9L61VRtkAc13wly/2SaO0f1pmH2NOWvczFvXlWtXl4hfzr0ZOrv47brY4qmjW0iocnkd/fF3V6yGt2Wyx5SFvZYBCqgCoM5SZb9O0IYvQevD8kZHJOUEUz5upvFreeRog7RlctatxxnzopVIDjxPVydtzry53W1oJqj8kLFMUc1yIodAJi8FuzQRiH root@master1" >> ~/.ssh/authorized_keys
```

* 4.2、下载二进制文件

* 4.3、分发文件并设置好PATH

```
tar -zxvf kubernetes-server-linux-amd641.11.3.tar.gz
cd /opt/kubernetes/docker/kubernetes/server/bin
rm *.* -f

# 创建目录
ssh 192.168.0.2 "mkdir -p /opt/kubernetes/bin"
ssh 192.168.0.3 "mkdir -p /opt/kubernetes/bin"
ssh 192.168.0.4 "mkdir -p /opt/kubernetes/bin"
ssh 192.168.0.5 "mkdir -p /opt/kubernetes/bin"
ssh 192.168.0.6 "mkdir -p /opt/kubernetes/bin"

# 分发文件
scp * -r 192.168.0.2:/opt/kubernetes/bin
scp * -r 192.168.0.3:/opt/kubernetes/bin
scp * -r 192.168.0.4:/opt/kubernetes/bin
scp * -r 192.168.0.5:/opt/kubernetes/bin
scp * -r 192.168.0.6:/opt/kubernetes/bin

# 给每个节点设置PATH
ssh 192.168.0.2 "echo 'PATH=/opt/kubernetes/bin:$PATH' >>/etc/profile"
ssh 192.168.0.3 "echo 'PATH=/opt/kubernetes/bin:$PATH' >>/etc/profile"
ssh 192.168.0.4 "echo 'PATH=/opt/kubernetes/bin:$PATH' >>/etc/profile"
ssh 192.168.0.5 "echo 'PATH=/opt/kubernetes/bin:$PATH' >>/etc/profile"
ssh 192.168.0.6 "echo 'PATH=/opt/kubernetes/bin:$PATH' >>/etc/profile"

# 在每个节点执行，更新环境变量
source /etc/profile
```

### 5、准备配置文件
* 5.1、下载配置文件
```
cd ~
git clone https://gitee.com/pa/kubernetes-ha-binary.git
ls -l kubernetes-ha-binary
cd kubernetes-ha-binary
vi global-config.properties
./init.sh
find target/ -type f
```


* 5.2、global-config.properties内容
```
# 3个master节点的ip
MASTER_0_IP=192.168.0.2
MASTER_1_IP=192.168.0.3
MASTER_2_IP=192.168.0.4

# 3个master节点的ip
MASTER_0_IP=192.168.0.2
MASTER_1_IP=192.168.0.3
MASTER_2_IP=192.168.0.4

# api-server的高可用虚拟ip
MASTER_VIP=192.168.0.100

# keepalived用到的网卡接口名，一般是eth0
VIP_IF=eth0

# worker节点的ip列表
WORKER_IPS=192.168.0.5,192.168.0.6

# kubernetes服务ip网段
SERVICE_CIDR=10.254.0.0/16

# kubernetes的默认服务ip，一般是cidr的第一个
KUBERNETES_SVC_IP=10.254.0.1

# dns服务的ip地址，一般是cidr的第二个
CLUSTER_DNS=10.254.0.2

# pod网段
POD_CIDR=172.22.0.0/16

# NodePort的取值范围
NODE_PORT_RANGE=8400-8900
```

### 6、CA证书

* 6.1、安装cfssl（主节点执行）

```
mkdir -p ~/bin
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O ~/bin/cfssl
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O ~/bin/cfssljson

# 修改为可执行权限
chmod +x ~/bin/cfssl ~/bin/cfssljson

# 设置PATH
vi /etc/profile
export PATH=$PATH:~/bin
source /etc/profile

# 验证
cfssl version
```

* 6.2、生成根证书（主节点执行）    
根证书是集群所有节点共享的，只需要创建一个 CA 证书，后续创建的所有证书都由它签名。

```
# 生成证书和私钥
cd /root/kubernetes-ha-binary/target/pki
cfssl gencert -initca ca-csr.json | cfssljson -bare ca

# 生成完成后会有以下文件（我们最终想要的就是ca-key.pem和ca.pem，一个秘钥，一个证书）
ls
ca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem

# 创建目录
ssh 192.168.0.2 "mkdir -p /etc/kubernetes/pki/"
ssh 192.168.0.3 "mkdir -p /etc/kubernetes/pki/"
ssh 192.168.0.4 "mkdir -p /etc/kubernetes/pki/"
ssh 192.168.0.5 "mkdir -p /etc/kubernetes/pki/"
ssh 192.168.0.6 "mkdir -p /etc/kubernetes/pki/"

# 分发到每个主节点
scp ca*.pem 192.168.0.2:/etc/kubernetes/pki/
scp ca*.pem 192.168.0.3:/etc/kubernetes/pki/
scp ca*.pem 192.168.0.4:/etc/kubernetes/pki/
```

### 7、部署ETCD集群（master节点）
* 7.1、下载etcd

```
# 下载etcd
cd ~
wget https://github.com/coreos/etcd/releases/download/v3.2.18/etcd-v3.2.18-linux-amd64.tar.gz
tar -zxvf etcd-v3.2.18-linux-amd64.tar.gz
cd etcd-v3.2.18-linux-amd64

# 分发文件
scp etcd etcdctl 192.168.0.2:/opt/kubernetes/bin
scp etcd etcdctl 192.168.0.3:/opt/kubernetes/bin
scp etcd etcdctl 192.168.0.4:/opt/kubernetes/bin
```

* 7.2、生成证书和私钥

```
cd /root/kubernetes-ha-binary/target/pki/etcd
cfssl gencert -ca=../ca.pem \
    -ca-key=../ca-key.pem \
    -config=../ca-config.json \
    -profile=kubernetes etcd-csr.json | cfssljson -bare etcd

# 分发到每个etcd节点（主节点）
scp etcd*.pem 192.168.0.2:/etc/kubernetes/pki/
scp etcd*.pem 192.168.0.3:/etc/kubernetes/pki/
scp etcd*.pem 192.168.0.4:/etc/kubernetes/pki/
```

* 7.3、创建service文件

```
cd /root/kubernetes-ha-binary/

# scp配置文件到每个master节点
scp target/192.168.0.2/services/etcd.service 192.168.0.2:/etc/systemd/system/
scp target/192.168.0.3/services/etcd.service 192.168.0.3:/etc/systemd/system/
scp target/192.168.0.4/services/etcd.service 192.168.0.4:/etc/systemd/system/

# 创建数据和工作目录
ssh 192.168.0.2 "mkdir -p /var/lib/etcd"
ssh 192.168.0.3 "mkdir -p /var/lib/etcd"
ssh 192.168.0.4 "mkdir -p /var/lib/etcd"
```

* 7.4、启动服务    
etcd 进程首次启动时会等待其它节点的 etcd 加入集群，命令 systemctl start etcd 会卡住一段时间，为正常现象。    

```
#启动服务
systemctl daemon-reload && systemctl enable etcd && systemctl restart etcd

#查看状态
service etcd status

#查看启动日志
journalctl -f -u etcd
```

### 8、部署API节点

* 8.1、生成证书和私钥

```
cd /root/kubernetes-ha-binary/target/pki/apiserver
cfssl gencert -ca=../ca.pem \
  -ca-key=../ca-key.pem \
  -config=../ca-config.json \
  -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes

# 分发到每个master节点
scp kubernetes*.pem 192.168.0.2:/etc/kubernetes/pki/
scp kubernetes*.pem 192.168.0.3:/etc/kubernetes/pki/
scp kubernetes*.pem 192.168.0.4:/etc/kubernetes/pki/
```

* 8.2、创建service文件

```
cd /root/kubernetes-ha-binary

# scp配置文件到每个master节点
scp target/192.168.0.2/services/kube-apiserver.service 192.168.0.2:/etc/systemd/system/
scp target/192.168.0.3/services/kube-apiserver.service 192.168.0.3:/etc/systemd/system/
scp target/192.168.0.4/services/kube-apiserver.service 192.168.0.4:/etc/systemd/system/

# 创建日志目录
ssh 192.168.0.2 "mkdir -p /var/log/kubernetes"
ssh 192.168.0.3 "mkdir -p /var/log/kubernetes"
ssh 192.168.0.4 "mkdir -p /var/log/kubernetes"
ssh 192.168.0.5 "mkdir -p /var/log/kubernetes"
ssh 192.168.0.6 "mkdir -p /var/log/kubernetes"
```

* 8.3、启动服务

```
#启动服务
systemctl daemon-reload && systemctl enable kube-apiserver && systemctl restart kube-apiserver

#查看运行状态
service kube-apiserver status

#查看日志
journalctl -f -u kube-apiserver

#检查监听端口
netstat -ntlp
```

### 9、部署keepalived - apiserver高可用（master节点）
* 9.1、安装keepalived

```
# 在两个主节点上安装keepalived（一主一备）
yum install -y keepalived
```

* 9.2、创建keepalived配置文件

```
ssh 192.168.0.2 "mkdir -p /etc/keepalived"
ssh 192.168.0.3 "mkdir -p /etc/keepalived"
cd /root/kubernetes-ha-binary/target/configs

# 分发配置文件
scp keepalived-master.conf 192.168.0.2:/etc/keepalived/keepalived.conf
scp keepalived-backup.conf 192.168.0.3:/etc/keepalived/keepalived.conf

# 分发监测脚本
scp check-apiserver.sh 192.168.0.2:/etc/keepalived/
scp check-apiserver.sh 192.168.0.3:/etc/keepalived/
```

* 9.3、启动服务

```
# 分别在master和backup上启动服务
systemctl enable keepalived && service keepalived start

# 检查状态
service keepalived status

# 查看日志
journalctl -f -u keepalived

# 访问测试
curl --insecure https://192.168.0.100:6443/
```

### 10、部署kubectl（任意节点）    
kubectl 是 kubernetes 集群的命令行管理工具，它默认从 ~/.kube/config 文件读取 kube-apiserver 地址、证书、用户名等信息。    
* 10.1、创建 admin 证书和私钥

```
# 创建证书、私钥
cd /root/kubernetes-ha-binary/target/pki/admin

cfssl gencert -ca=../ca.pem \
  -ca-key=../ca-key.pem \
  -config=../ca-config.json \
  -profile=kubernetes admin-csr.json | cfssljson -bare admin
```

* 10.2、创建kubeconfig配置文件
kubeconfig 为 kubectl 的配置文件，包含访问 apiserver 的所有信息，如 apiserver 地址、CA 证书和自身使用的证书

```
# 设置集群参数
kubectl config set-cluster kubernetes \
  --certificate-authority=../ca.pem \
  --embed-certs=true \
  --server=https://192.168.0.100:6443 \
  --kubeconfig=kube.config

# 设置客户端认证参数
kubectl config set-credentials admin \
  --client-certificate=admin.pem \
  --client-key=admin-key.pem \
  --embed-certs=true \
  --kubeconfig=kube.config

# 设置上下文参数
kubectl config set-context kubernetes \
  --cluster=kubernetes \
  --user=admin \
  --kubeconfig=kube.config

# 设置默认上下文
kubectl config use-context kubernetes --kubeconfig=kube.config

# 创建目录
ssh 192.168.0.2 "mkdir -p ~/.kube/config"
ssh 192.168.0.3 "mkdir -p ~/.kube/config"
ssh 192.168.0.4 "mkdir -p ~/.kube/config"
ssh 192.168.0.5 "mkdir -p ~/.kube/config"
ssh 192.168.0.6 "mkdir -p ~/.kube/config"

# 分发到目标节点
scp kube.config 192.168.0.2:~/.kube/config
scp kube.config 192.168.0.3:~/.kube/config
scp kube.config 192.168.0.4:~/.kube/config
scp kube.config 192.168.0.5:~/.kube/config
scp kube.config 192.168.0.6:~/.kube/config
```

* 10.3、授予 kubernetes 证书访问 kubelet API 的权限    
在执行 kubectl exec、run、logs 等命令时，apiserver 会转发到 kubelet。这里定义 RBAC 规则，授权 apiserver 调用 kubelet API。

```
kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes
```

* 10.4、检测集群状态

```
# 查看集群信息
kubectl cluster-info
kubectl get all --all-namespaces
kubectl get componentstatuses
```

### 11、部署controller-manager（master节点）
controller-manager启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。    
* 11.1、创建证书和私钥

```
# 生成证书、私钥
cd /root/kubernetes-ha-binary/target/pki/controller-manager
cfssl gencert -ca=../ca.pem \
  -ca-key=../ca-key.pem \
  -config=../ca-config.json \
  -profile=kubernetes controller-manager-csr.json | cfssljson -bare controller-manager

# 分发到每个master节点
scp controller-manager*.pem 192.168.0.2:/etc/kubernetes/pki/
scp controller-manager*.pem 192.168.0.3:/etc/kubernetes/pki/
scp controller-manager*.pem 192.168.0.4:/etc/kubernetes/pki/
```

* 11.2、创建controller-manager的kubeconfig

```
# 创建kubeconfig
kubectl config set-cluster kubernetes \
  --certificate-authority=../ca.pem \
  --embed-certs=true \
  --server=https://192.168.0.100:6443 \
  --kubeconfig=controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=controller-manager.pem \
  --client-key=controller-manager-key.pem \
  --embed-certs=true \
  --kubeconfig=controller-manager.kubeconfig

kubectl config set-context system:kube-controller-manager \
  --cluster=kubernetes \
  --user=system:kube-controller-manager \
  --kubeconfig=controller-manager.kubeconfig

kubectl config use-context system:kube-controller-manager --kubeconfig=controller-manager.kubeconfig

# 分发controller-manager.kubeconfig
scp controller-manager.kubeconfig 192.168.0.2:/etc/kubernetes/
scp controller-manager.kubeconfig 192.168.0.3:/etc/kubernetes/
scp controller-manager.kubeconfig 192.168.0.4:/etc/kubernetes/
```

* 11.3、创建service文件

```
cd /root/kubernetes-ha-binary/

# scp配置文件到每个master节点
scp target/services/kube-controller-manager.service 192.168.0.2:/etc/systemd/system/
scp target/services/kube-controller-manager.service 192.168.0.3:/etc/systemd/system/
scp target/services/kube-controller-manager.service 192.168.0.4:/etc/systemd/system/
```

* 11.4、启动服务

```
# 启动服务
systemctl daemon-reload && systemctl enable kube-controller-manager && systemctl restart kube-controller-manager

# 检查状态
service kube-controller-manager status

# 查看日志
journalctl -f -u kube-controller-manager

# 查看leader
kubectl get endpoints kube-controller-manager --namespace=kube-system -o yaml
```

### 12、部署scheduler（master节点）
scheduler启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。    
* 12.1、创建证书和私钥

```
# 生成证书、私钥
cd /root/kubernetes-ha-binary/target/pki/scheduler
cfssl gencert -ca=../ca.pem \
  -ca-key=../ca-key.pem \
  -config=../ca-config.json \
  -profile=kubernetes scheduler-csr.json | cfssljson -bare kube-scheduler
```
* 12.2、创建scheduler的kubeconfig
```
# 创建kubeconfig
kubectl config set-cluster kubernetes \
  --certificate-authority=../ca.pem \
  --embed-certs=true \
  --server=https://192.168.0.100:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.pem \
  --client-key=kube-scheduler-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context system:kube-scheduler \
  --cluster=kubernetes \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig
kubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig

# 分发kubeconfig
scp kube-scheduler.kubeconfig 192.168.0.2:/etc/kubernetes/
scp kube-scheduler.kubeconfig 192.168.0.3:/etc/kubernetes/
scp kube-scheduler.kubeconfig 192.168.0.4:/etc/kubernetes/
```

* 12.3、创建service文件
```
cd /root/kubernetes-ha-binary/
# scp配置文件到每个master节点
scp target/services/kube-scheduler.service 192.168.0.2:/etc/systemd/system/
scp target/services/kube-scheduler.service 192.168.0.3:/etc/systemd/system/
scp target/services/kube-scheduler.service 192.168.0.4:/etc/systemd/system/
```

* 12.4、启动服务
```
# 启动服务
systemctl daemon-reload && systemctl enable kube-scheduler && systemctl restart kube-scheduler

# 检查状态
service kube-scheduler status

# 查看日志
journalctl -f -u kube-scheduler

# 查看leader
kubectl get endpoints kube-scheduler --namespace=kube-system -o yaml
```

### 13、部署kubelet（worker节点）
* 13.1、预先下载需要的镜像
```
cd /root/kubernetes-ha-binary/

# 预先下载镜像到所有节点（由于镜像下载的速度过慢，我给大家提供了阿里云仓库的镜像）
scp target/configs/download-images.sh 192.168.0.5:~
scp target/configs/download-images.sh 192.168.0.6:~

# 在目标节点上执行脚本下载镜像
sh ~/download-images.sh
```

* 13.2、创建bootstrap配置文件
```
# 创建 token
cd /root/kubernetes-ha-binary/target/pki/admin
export BOOTSTRAP_TOKEN=$(kubeadm token create \
      --description kubelet-bootstrap-token \
      --groups system:bootstrappers:worker \
      --kubeconfig kube.config)

# 设置集群参数
kubectl config set-cluster kubernetes \
      --certificate-authority=../ca.pem \
      --embed-certs=true \
      --server=https://192.168.0.100:6443 \
      --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置客户端认证参数
kubectl config set-credentials kubelet-bootstrap \
      --token=${BOOTSTRAP_TOKEN} \
      --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置上下文参数
kubectl config set-context default \
      --cluster=kubernetes \
      --user=kubelet-bootstrap \
      --kubeconfig=kubelet-bootstrap.kubeconfig

# 设置默认上下文
kubectl config use-context default --kubeconfig=kubelet-bootstrap.kubeconfig

# 把生成的配置copy到每个worker节点上
scp kubelet-bootstrap.kubeconfig 192.168.0.5:/etc/kubernetes/kubelet-bootstrap.kubeconfig
scp kubelet-bootstrap.kubeconfig 192.168.0.6:/etc/kubernetes/kubelet-bootstrap.kubeconfig

# 先在worker节点上创建目录
ssh 192.168.0.5 "mkdir -p /etc/kubernetes/pki"
ssh 192.168.0.6 "mkdir -p /etc/kubernetes/pki"

# 把ca分发到每个worker节点
cd /root/kubernetes-ha-binary/
scp target/pki/ca.pem 192.168.0.5:/etc/kubernetes/pki/
scp target/pki/ca.pem 192.168.0.6:/etc/kubernetes/pki/
```

* 13.3、kubelet配置文件
把kubelet配置文件分发到每个worker节点上    
```
scp target/worker-192.168.0.5/kubelet.config.json 192.168.0.5:/etc/kubernetes/
scp target/worker-192.168.0.6/kubelet.config.json 192.168.0.6:/etc/kubernetes/
```

* 13.4、kubelet服务文件
把kubelet服务文件分发到每个worker节点上    
```
scp target/worker-192.168.0.5/kubelet.service 192.168.0.5:/etc/systemd/system/
scp target/worker-192.168.0.6/kubelet.service 192.168.0.6:/etc/systemd/system/
```

* 13.5、启动服务    
kublet 启动时查找配置的 --kubeletconfig 文件是否存在，如果不存在则使用 --bootstrap-kubeconfig 向 kube-apiserver 发送证书签名请求 (CSR)。 kube-apiserver 收到 CSR 请求后，对其中的 Token 进行认证（事先使用 kubeadm 创建的 token），认证通过后将请求的 user 设置为 system:bootstrap:，group 设置为 system:bootstrappers，这就是Bootstrap Token Auth。    
```
# bootstrap附权(master节点)
kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers

# 启动服务
mkdir -p /var/lib/kubelet
systemctl daemon-reload && systemctl enable kubelet && systemctl restart kubelet

# 在master上Approve bootstrap请求
kubectl get csr
kubectl certificate approve node-csr-URxWWOJKyYkzOlzlVag63U25_G_R0vxZsYiyzyhEsPQ
kubectl certificate approve node-csr-z4h4nkz9wbCzt4EPCzAKwdj4rQEZELn9ikVQTEAuQjg

# 查看服务状态
service kubelet status

# 查看日志
journalctl -f -u kubelet
```

### 14、部署kube-proxy（worker节点）
* 14.1、创建证书和私钥
```
cd /root/kubernetes-ha-binary/target/pki/proxy

cfssl gencert -ca=../ca.pem \
  -ca-key=../ca-key.pem \
  -config=../ca-config.json \
  -profile=kubernetes  kube-proxy-csr.json | cfssljson -bare kube-proxy
```

* 14.2、创建和分发 kubeconfig 文件
```
# 创建kube-proxy.kubeconfig
kubectl config set-cluster kubernetes \
  --certificate-authority=../ca.pem \
  --embed-certs=true \
  --server=https://192.168.0.100:6443 \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy \
  --client-certificate=kube-proxy.pem \
  --client-key=kube-proxy-key.pem \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes \
  --user=kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

# 分发kube-proxy.kubeconfig
scp kube-proxy.kubeconfig 192.168.0.5:/etc/kubernetes/
scp kube-proxy.kubeconfig 192.168.0.6:/etc/kubernetes/
```

* 14.3、分发kube-proxy.config
```
cd /root/kubernetes-ha-binary/
scp target/worker-192.168.0.5/kube-proxy.config.yaml 192.168.0.5:/etc/kubernetes/
scp target/worker-192.168.0.6/kube-proxy.config.yaml 192.168.0.6:/etc/kubernetes/
```

* 14.4、分发kube-proxy服务文件
```
scp target/services/kube-proxy.service 192.168.0.5:/etc/systemd/system/
scp target/services/kube-proxy.service 192.168.0.6:/etc/systemd/system/
```

* 14.5、启动服务
```
# 创建依赖目录
mkdir -p /var/lib/kube-proxy && mkdir -p /var/log/kubernetes

# 启动服务
systemctl daemon-reload && systemctl enable kube-proxy && systemctl restart kube-proxy

# 查看状态
service kube-proxy status

# 查看日志
journalctl -f -u kube-proxy
```

### 15、部署CNI插件 - calico    
使用calico官方的安装方式来部署。    

```
# 创建目录（在配置了kubectl的节点上执行）
mkdir -p /etc/kubernetes/addons
cd /root/kubernetes-ha-binary/

# 上传calico配置到配置好kubectl的节点（一个节点即可）
scp target/addons/calico* 192.168.0.2:/etc/kubernetes/addons/

# 部署calico
kubectl create -f /etc/kubernetes/addons/calico-rbac-kdd.yaml
kubectl create -f /etc/kubernetes/addons/calico.yaml

# 查看状态
kubectl get pods -n kube-system
```

### 16、部署DNS插件 - coredns
```
# 上传配置文件
scp target/addons/coredns.yaml 192.168.0.2:/etc/kubernetes/addons/

# 部署coredns
kubectl create -f /etc/kubernetes/addons/coredns.yaml
kubectl get pods
```

### 17、集群可用性测试
* 17.1、创建nginx ds   

```
cat > nginx-ds.yml <<EOF
apiVersion: v1
kind: Service
metadata:
  name: nginx-ds
  labels:
    app: nginx-ds
spec:
  type: NodePort
  selector:
    app: nginx-ds
  ports:
  - name: http
    port: 80
    targetPort: 80
---
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: nginx-ds
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
spec:
  template:
    metadata:
      labels:
        app: nginx-ds
    spec:
      containers:
      - name: my-nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
EOF

# 创建ds
kubectl create -f nginx-ds.yml
```

* 17.2、检查各种ip连通性   

```
# 检查各 Node 上的 Pod IP 连通性（主节点没有calico所以不能访问podip）
kubectl get pods  -o wide

# 在每个worker节点上ping pod ip
ping 172.22.1.2
ping 172.22.0.3

# 检查service可达性
kubectl get svc

# 在每个worker节点上访问服务（主节点没有proxy所以不能访问service-ip）
curl 10.254.52.9:80

# 在每个节点检查node-port可用性
curl 192.168.0.5:8582
curl 192.168.0.6:8582
```

* 17.3、检查dns可用性

```
# 创建一个nginx pod
cat > pod-nginx.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  containers:
  - name: nginx
    image: nginx:1.7.9
    ports:
    - containerPort: 80
EOF

# 创建pod
kubectl create -f pod-nginx.yaml

# 进入pod，查看dns
kubectl exec  nginx -i -t -- /bin/bash

# 查看dns配置
root@nginx:/# cat /etc/resolv.conf

# 查看名字是否可以正确解析
root@nginx:/# ping nginx-ds
root@nginx:/# ping kubernetes
```

### 18、部署dashboard
* 18.1、部署dashboard

```
cd /root/kubernetes-ha-binary/
# 上传dashboard配置
scp target/addons/dashboard-all.yaml 192.168.0.2:/etc/kubernetes/addons/

# 创建服务
kubectl apply -f /etc/kubernetes/addons/dashboard-all.yaml

# 查看服务运行情况
kubectl get services kubernetes-dashboard -n kube-system
kubectl get deployment kubernetes-dashboard -n kube-system
kubectl --namespace kube-system get pods -o wide
kubectl get services kubernetes-dashboard -n kube-system
netstat -ntlp|grep 8401
```

* 18.2、访问dashboard
为了集群安全，从 1.7 开始，dashboard 只允许通过 https 访问，我们使用nodeport的方式暴露服务，可以使用 https://NodeIP:NodePort 地址访问 关于自定义证书 默认dashboard的证书是自动生成的，肯定是非安全的证书，如果大家有域名和对应的安全证书可以自己替换掉。使用安全的域名方式访问dashboard。 在dashboard-all.yaml中增加dashboard启动参数，可以指定证书文件，其中证书文件是通过secret注进来的。    

```
- –tls-cert-file
- dashboard.cer
- –tls-key-file
- dashboard.key
```

* 18.3、登录dashboard
Dashboard 默认只支持 token 认证，所以如果使用 KubeConfig 文件，需要在该文件中指定 token，我们这里使用token的方式登录    

```
# 创建service account
kubectl create sa dashboard-admin -n kube-system

# 创建角色绑定关系
kubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-admin

# 查看dashboard-admin的secret名字
ADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk '{print $1}')

# 打印secret的token
kubectl describe secret -n kube-system ${ADMIN_SECRET} | grep -E '^token' | awk '{print $2}'
```

### 部署Fabric
* kompose 进行转换compose文件为k8s可使用的文件

```
cd /opt
mkdir hyfa && cd hyfa
wget https://raw.githubusercontent.com/hyperledger/fabric/master/scripts/bootstrap.sh
chmod +x bootstrap.sh
./bootstrap.sh 1.2.0

# 上传kompose
rz
tar -zxvf kompose-linux-amd64.tar.gz
mv kompose-linux-amd64 /opt/kubernetes/bin/

kompose convert -f docker-complse-cli.yaml --volumes hostPath
kompose convert -f docker-compose-couch.yaml --volumes hostsPath
kompose convert -f dc-orderer-kafka.yml --volumes hostsPath

```
