
ceph组件 mds
存储ceph文件系统的元数据
对外提供cephfs

osd
osd守护进程
用于存储数据、处理数据拷贝、恢复、回滚、均衡
通过心跳程序想monitor提供部分监控信息
一个Ceph集群至少需要两个osd守护进程

mon
维护集群的状态映射信息
包括monitor,osd,placement group
维护状态改变和历史信息

mgr
负载ceph集群管理入pg map
对外提供集群性能指标
具有web界面的监控系统（dashboard）

ceph生态架构
client -> metadata server cluster -> object stage cluster -> filesystem

通过tgt支持iscsi挂载

海量文件、大流量、高并发
需要高可用、高性能文件系统
传统但服务器及NAS共享难以满足需求，如存储容量高可用

通过cephfs
多服务器间的文件共享
如日志服务器 统一收集存放
如Web服务器文件统一挂载

容器环境中的数据持久化
通过cephfs
通过rbd 绑定容器

生产环境推荐配置
存储集群采用全万兆网络
集群网络 与 公共网络分离
mon mds 与 osd分离部署在不同机器上
journal 推荐使用pci ssd ，
osd使用sata即可
根据容量规划集群
E5 2620V3 64G以上的内存
集群主机分散部署，避免机柜故障（电源、网络）

最小化配置
ceph-0 eth0: 192.168.50.20
       eth1: 172.20.0.20 (用于内部通讯)
       角色：mon,osd,mgr
ceph-1 eth0: 192.168.50.21
      eth1: 172.20.0.21
      角色：mon,osd,mgr
ceph-2 eth0: 192.168.50.22
      eth1: 172.20.0.22
      角色：mon,osd,mgr

版本 luminous 12.2
CPU 2核心 4GB内存

192.168.50.0/24 公共网络 提供外部访问
172.20.0.0/24 集群网络不对外

磁盘
disk 0 /dev/vda 20G OS
disk 1 /dev/vdb 10G Journal
disk 2 /dev/vdc 20G OSD
disk 3 /dev/vdd 20G OSD


安装前环境准备
df -lh
ping ceph-1 (主机名需要能ping通)
或者用host方式
vi /etc/hosts
192.168.50.20 ceph-0
192.168.50.21 ceph-1
192.168.50.22 ceph-2

cd .ssh/
cd /root
mkdir -m 700 .ssh
ssh-keygen -t rsa -b 3072
id_rsa_ceph
ssh-copy-id ceph-0
ssh-copy-id ceph-1
ssh-copy-id ceph-2

ssh ceph-0
ssh-add .ssh/id_rsa_ceph
ssh -i .ssh/id_rsa_ceph ceph-0
ssh-add id_rsa_ceph

firewall-cmd --zone=public --add-port=6789/tcp --permanent
firewall-cmd --zone=public --add-port=6800-7100/tcp --permanent
firewall-cmd --reload

yum install -y ntpdate
ntpdate cn.pool.ntp.org

vi /etc/crontab
00 00 * * * root /usr/sbin/ntpdate cn.pool.ntp.org

rpm -ivh epel源

安装ceph源
yum install -y ceph-deploy
mdkir -p /data/ceph-install/
ll /dev/vd*
parted /dev/vdb
mklabel gpt
mkpart primary xfs 0% 50%
mkpart primary xfs 50% 100%
p


yum install -y pip
ceph-deploy new ceph-0 ceph-1 ceph-2
ceph-deploy install ceph-0 ceph-1 ceph-2
yum makecache

yum clean all
yum install -y ceph

创建监控节点
ceph-deploy mon create-initial   
ceph -s
# ceph-deploy disk zap ceph-0 vdc vdb
ceph-deploy osd create ceph-0 --data /dev/vdc --journal /dev/vdb1
ceph-deploy osd create ceph-0 --data /dev/vdd --journal /dev/vdb2

ceph-deploy osd create ceph-1 --data /dev/vdc --journal /dev/vdb1
ceph-deploy osd create ceph-1 --data /dev/vdd --journal /dev/vdb2

ceph-deploy osd create ceph-2 --data /dev/vdc --journal /dev/vdb1
ceph-deploy osd create ceph-2 --data /dev/vdd --journal /dev/vdb2

ceph-deploy --overwrite-conf admin ceph-0 ceph-1 ceph-2
ceph -s

ceph-deploy mgr create ceph-0:ceph-0 ceph-1:ceph-1 ceph-2:ceph-2

创建存储池和文件系统
ceph-deploy mds create ceph-0 ceph-1 ceph-2
ceph -s

vi ceph.conf
```
mon_initial_members = ceph-0, ceph-1, ceph-2
mon_host = 192.168.50.20,192.168.50.21,192.168.50.22

xxxxxx
复制配置文件

public network = 192.168.50.0/24
cluster network = 172.20.0.0/24

```
ceph-deploy --overwrite-conf admin ceph-0 ceph-1 ceph-2

systemctl restart ceph\*.service ceph\*.target
ceph -s

创建存储池
ceph osd pool ls
ceph fs ls
ceph osd pool create data_data 32 （名称data，后面的data数据池）32是数量
ceph osd poll create data_metadata 32
ceph osd pool ls
cepf fs new data data_metadata data_data
cepf fs ls
ceph -s

挂载fs,55
yum install -y ceph-fuse
cd /
mdkir data
ping ceph-0
ping ceph-1
ping ceph-2
ceph-fuse -m ceph-0,ceph-1,ceph-2:6789 /data

cd /etc/ceph
mkdir -p /etc/ceph
vi /etc/ceph/ceph.conf
把之前集群的配置复制过来保存

ceph auth ls
创建一个权限比较小的认证用户给客户端使用
ceph auth add client.user osd 'allow r' mon 'allow r' mds 'allow r'
ceph auth caps client.user mds 'allow r' mon 'allow r' osd 'allow rw'
ceph auth --help
cat ceph.client.admin.keyring

vi /etc/ceph/ceph.keyring
[client.admin]
  key = xxxxxxxxxxxxx

ceph-fuse -m ceph-0 ceph-1 ceph-2:6789 /data

cd /data

* 扩容
mkdir .ssh -m 700
加入ssh免密

parted /dev/vdb
mklabel gpt
mkpart primary xfs 0% 50%
mkpart primary xfs 50% 100%
p
firewall-cmd --zone=public --add-port=6789/tcp --permanent
firewall-cmd --zone=public --add-port=6800-6810/tcp --permanent
firewall-cmd --reload
yum install ntpdate
ntpdate cn.pool.ntp.org

yum install epel源
yum install -y ceph

添加新的OSD
ceph-deploy osd create ceph-3 --data /dev/vdc --journal /dev/vdb1
ceph-deploy osd create ceph-3 --data /dev/vdd --journal /dev/vdb2
ceph -s


ceph osd pool ls
ceph osd pool create data 128
ceph osd pool ls
